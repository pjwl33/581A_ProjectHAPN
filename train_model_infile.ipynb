{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import TensorFlow and other necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Import for predictions\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "# Supressing warning messages on output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Visualize the Training Results in Plot Data\n",
    "def visualization_report(history, epochs, model_name):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Val. Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f'Train and Val. Accuracy: {model_name}')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Val. Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f'Train and Val. Loss: {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_predictions(model, class_names, img_height, img_width, config):\n",
    "    # Running Predictions on Images outside of Dataset in General\n",
    "    pred_path = config[\"prediction_data_path\"]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    figure_counter = 1\n",
    "\n",
    "    for img_p in os.listdir(pred_path):\n",
    "        if (img_p.endswith(\".png\")) or (img_p.endswith(\".jpg\")) or (img_p.endswith(\".jpeg\")):\n",
    "            print(img_p)\n",
    "            # Grayscale the image in the same way for prediction\n",
    "            img_path = f'{pred_path}{img_p}'\n",
    "            img = tf.keras.utils.load_img(\n",
    "                img_path, color_mode='grayscale', target_size=(img_height, img_width), keep_aspect_ratio=True\n",
    "            )\n",
    "\n",
    "            ax = plt.subplot(3, 3, figure_counter)\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            img_array = tf.keras.utils.img_to_array(img)\n",
    "            img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "\n",
    "            predictions = model.predict(img_array)\n",
    "            score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "            result_msg = f'{class_names[np.argmax(score)]}, {100 * np.max(score)}'\n",
    "            plt.title(result_msg)\n",
    "            print(\n",
    "                \"{} most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "                .format(img_p, class_names[np.argmax(score)], 100 * np.max(score))\n",
    "            )\n",
    "\n",
    "            figure_counter += 1\n",
    "\n",
    "def train_keras_model(config={}):\n",
    "\n",
    "    # Requires Config\n",
    "    if not bool(config):\n",
    "        return None\n",
    "\n",
    "    # Image Loader Params\n",
    "    shape_size  = 1\n",
    "    dataset_url = config[\"dataset_path\"]\n",
    "    batch_size  = config[\"batch_size\"]\n",
    "    batch_seed  = config[\"batch_seed\"]\n",
    "    img_height  = config[\"img_height\"]\n",
    "    img_width   = config[\"img_width\"]\n",
    "    img_shape   = (img_width, img_height)\n",
    "    input_shape = (img_width, img_height, shape_size)\n",
    "\n",
    "    tf_autotune = tf.data.AUTOTUNE\n",
    "    data_dir = pathlib.Path(dataset_url)\n",
    "\n",
    "    # Train Split at 70%\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.3,\n",
    "        subset=\"training\",\n",
    "        image_size=img_shape,\n",
    "        seed=batch_seed,\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # Validation of the data at 70%\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.3,\n",
    "        subset=\"validation\",\n",
    "        image_size=img_shape,\n",
    "        seed=batch_seed,\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    # Classification of names - each image type in its own dir\n",
    "    class_names = train_ds.class_names\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf_autotune)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf_autotune)\n",
    "\n",
    "    # Create basic Keras Model\n",
    "\n",
    "    # The Keras Sequential model consists of three convolution blocks(tf.keras.layers.Conv2D)\n",
    "    # with a max pooling layer(tf.keras.layers.MaxPooling2D) in each of them.\n",
    "    #\n",
    "    # There's a fully-connected layer (tf.keras.layers.Dense) with 128 units on top of it that\n",
    "    # is activated by a ReLU activation function ('relu').\n",
    "    #\n",
    "    # This model has not been tuned for high accuracy; the goal of this tutorial is to show a standard approach.\n",
    "\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    model_save_path = config[\"model_save_path\"]\n",
    "    model_save_name = config[\"model_save_name\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    # try:\n",
    "    # model = keras.models.load_model(model_save_path)\n",
    "    # history = model.history\n",
    "    # except OSError:\n",
    "    model = Sequential([\n",
    "        Rescaling(1./255, input_shape=input_shape),\n",
    "        Conv2D(16, shape_size, activation='relu',\n",
    "               padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(32, shape_size, activation='relu',\n",
    "               padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, shape_size, activation='relu',\n",
    "               padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "\n",
    "    # For this tutorial, choose the tf.keras.optimizers.Adam optimizer and\n",
    "    # tf.keras.losses.SparseCategoricalCrossentropy loss function.\n",
    "    #\n",
    "    # To view training and validation accuracy for each training epoch, pass the metrics argument to Model.compile.\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['acc'])\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    model.save(model_save_path)\n",
    "    print(f'Model Summary: {model_save_name}', model.summary())\n",
    "    \n",
    "    visualization_report(history, epochs, model_save_name)\n",
    "    run_predictions(model, class_names, img_height, img_width, config)\n",
    "\n",
    "# All different types of config to be generated\n",
    "def generate_configs(model_type):\n",
    "    # Generate configs by model type/name\n",
    "    return {\n",
    "        \"dataset_path\": f'data/training_{model_type}/',\n",
    "        \"model_save_path\": f'model/trained_{model_type}',\n",
    "        \"model_save_name\": f'trained_{model_type}',\n",
    "        \"prediction_data_path\": f'data/predictions/{model_type}/',\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 64,\n",
    "        \"batch_seed\": random.randint(1, 581),\n",
    "        \"img_height\": 120,\n",
    "        \"img_width\": 128\n",
    "    }\n",
    "\n",
    "\n",
    "# Human vs. Nonhuman Configurations\n",
    "config_human_nonhuman = generate_configs(\"human_nonhuman\")\n",
    "\n",
    "# Mask vs. No Mask Configurations\n",
    "config_mask_nomask = generate_configs(\"mask_nomask\")\n",
    "\n",
    "# Glasses vs No Glasses Configurations\n",
    "config_glasses_noglasses = generate_configs(\"glasses_noglasses\")\n",
    "\n",
    "train_keras_model(config_human_nonhuman)\n",
    "train_keras_model(config_mask_nomask)\n",
    "train_keras_model(config_glasses_noglasses)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.0 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}